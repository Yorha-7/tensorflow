{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bbb5b15-1037-4b53-aa46-46cb535cc53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ff\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"ff\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lenet_model_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LenetModel</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,407,425</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lenet_model_12 (\u001b[38;5;33mLenetModel\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │     \u001b[38;5;34m5,407,425\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,407,425</span> (20.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,407,425\u001b[0m (20.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,406,973</span> (20.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,406,973\u001b[0m (20.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">452</span> (1.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m452\u001b[0m (1.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 06:31:08.187590: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732948268.265666     671 service.cc:148] XLA service 0x7fdb64012c70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1732948268.266716     671 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2024-11-30 06:31:08.330880: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  7/517\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.5899 - loss: 0.9026"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1732948271.217768     671 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.6604 - loss: 0.6374 - val_accuracy: 0.6627 - val_loss: 0.8685\n",
      "Epoch 2/3\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.8750 - loss: 0.3022 - val_accuracy: 0.8938 - val_loss: 0.3143\n",
      "Epoch 3/3\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.9277 - loss: 0.1949 - val_accuracy: 0.9225 - val_loss: 0.2389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fdb680e6e70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Input, Dense, Conv2D, MaxPool2D, Flatten, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset\n",
    "dataset, dataset_info = tfds.load('malaria', with_info=True, as_supervised=True, shuffle_files=True, split='train')\n",
    "\n",
    "def reshape_resize(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    label = tf.cast(label, tf.float32) \n",
    "    return image, label\n",
    "\n",
    "# Split the dataset\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "LEN = len(dataset)\n",
    "\n",
    "train_size = int(LEN * TRAIN_RATIO)\n",
    "val_size = int(LEN * VAL_RATIO)\n",
    "\n",
    "train_ds = dataset.take(train_size)\n",
    "remaining_ds = dataset.skip(train_size)\n",
    "val_ds = remaining_ds.take(val_size)\n",
    "test_ds = remaining_ds.skip(val_size)\n",
    "\n",
    "# Preprocess the datasets\n",
    "train_ds = train_ds.map(reshape_resize).batch(32)\n",
    "val_ds = val_ds.map(reshape_resize).batch(32)\n",
    "test_ds = test_ds.map(reshape_resize).batch(32)\n",
    "\n",
    "class FeatureExtractor(Layer):\n",
    "    def __init__(self, filters_1, filters_2, kernel_size, strides, padding, activation, pool_size):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.conv_1 = Conv2D(filters=filters_1, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation)\n",
    "        self.batch_1 = BatchNormalization()\n",
    "        self.pool_1 = MaxPool2D(pool_size=pool_size, strides=strides*2)\n",
    "        self.conv_2 = Conv2D(filters=filters_2, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation)\n",
    "        self.batch_2 = BatchNormalization()\n",
    "        self.pool_2 = MaxPool2D(pool_size=pool_size, strides=strides*2)\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.batch_1(x, training=training)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.batch_2(x, training=training)\n",
    "        x = self.pool_2(x)\n",
    "        return x\n",
    "\n",
    "class LenetModel(Model):\n",
    "    def __init__(self):\n",
    "        super(LenetModel, self).__init__()\n",
    "        self.feature_extractor = FeatureExtractor(6, 16, (5, 5), 1, \"valid\", \"relu\", (2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(120, activation=\"relu\")\n",
    "        self.batch_1 = BatchNormalization()\n",
    "        self.dense_2 = Dense(84, activation=\"relu\")\n",
    "        self.batch_2 = BatchNormalization()\n",
    "        self.output_layer = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        x = self.feature_extractor(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.batch_1(x, training=training)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.batch_2(x, training=training)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "        \n",
    "# Create the model\n",
    "func_input = Input(shape=(224, 224, 3), name=\"input\")\n",
    "feature_sub_classed = LenetModel()\n",
    "func_output = feature_sub_classed(func_input)\n",
    "model = Model(inputs=func_input, outputs=func_output, name=\"ff\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea870d-8ca0-45fa-8986-3c22a33f1519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
